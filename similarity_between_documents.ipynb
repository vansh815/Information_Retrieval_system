{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h85arlny4zNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2euzn42b5DK8",
        "colab_type": "code",
        "outputId": "7f7a02d1-1b21-4ff9-e0bd-7de0465b2f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "doc1 = \"Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains\"\n",
        "doc2 = \"An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain.\"\n",
        "doc3 = \"In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.\"\n",
        "query = \"neuron in brain\"\n",
        "docs = doc1 + doc2 + doc3\n",
        "stop = set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "\n",
        "\n",
        "def CreateVocab(*docv):\n",
        "    core_words = []\n",
        "    for doc in docv:\n",
        "        for word in doc.split(\" \"):\n",
        "            if word.lower() not in stop:\n",
        "              core_words.append(word.lower())\n",
        "    return list(set(core_words))\n",
        "\n",
        "vocab = CreateVocab(doc1,doc2,doc3)\n",
        "\n",
        "print(vocab)\n",
        "vocab1 = word_tokenize(docs)\n",
        "print(vocab1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "{'she', 'where', \"you're\", 'yours', 'before', 's', 'on', 'too', \"needn't\", \"doesn't\", \"shouldn't\", 'hadn', 'if', 'for', 'of', 'wasn', 'by', 'doesn', 'from', 'what', 'few', 'once', 'him', 'or', 'no', 'shouldn', 'very', 'into', 'shan', \"don't\", 'hasn', 'been', \"weren't\", 'were', 'did', 'me', 'this', \"you'll\", 'the', 'i', 'he', 'then', 'they', 'not', 'when', \"haven't\", 'because', 'during', \"mightn't\", 'yourself', 'do', 'further', \"didn't\", 'mightn', 'does', 'y', \"you've\", 'down', 'yourselves', 'doing', \"it's\", 'its', 'whom', 'isn', 'd', \"aren't\", 'have', 'is', 'here', 'won', 'ours', 'our', 'a', 'all', \"mustn't\", 'above', 'but', \"wouldn't\", 'so', 'again', 'to', \"hasn't\", 'and', 'don', 'who', \"that'll\", \"you'd\", 'with', 'as', 'can', 'your', 'his', 'over', 'each', 'out', 'while', 'off', 'most', 'some', 'hers', 'other', 'through', 'we', 'wouldn', \"isn't\", 'll', 'her', 'be', 've', 'aren', 'herself', 'my', 'being', 're', \"she's\", 'didn', 'couldn', 'against', 'same', 'ourselves', 'was', 'you', 'up', 'weren', 'there', 'now', \"won't\", 'at', 'those', 'about', 'own', 'himself', 'these', 'themselves', 'an', 'only', 'after', 'will', 'm', 'myself', 'than', 'ain', 'how', 'why', 'more', 'in', \"wasn't\", 'nor', \"hadn't\", 'such', 'ma', 'between', 'mustn', 'until', 'has', 'under', \"couldn't\", 'just', 'them', 'below', 'their', 'itself', 'are', 'am', 'needn', 'both', 'theirs', 'having', \"should've\", 'that', 'had', \"shan't\", 'any', 't', 'which', 'haven', 'o', 'should', 'it'}\n",
            "['model', 'function', 'neural', 'vaguely', 'implementations,', 'signal', 'biological', 'computing', 'number,', 'real', 'non-linear', 'ann', 'connection', 'connected', 'inspired', 'networks', 'neurons', 'based', 'systems', 'brain.', 'common', 'computed', 'loosely', 'constitute', 'neuron', 'sum', 'animal', 'output', 'brains', 'called', 'nodes', 'artificial', 'inputs.', 'units', 'connectionist', '(ann)', 'collection']\n",
            "['Artificial', 'neural', 'networks', '(', 'ANN', ')', 'or', 'connectionist', 'systems', 'are', 'computing', 'systems', 'vaguely', 'inspired', 'by', 'the', 'biological', 'neural', 'networks', 'that', 'constitute', 'animal', 'brainsAn', 'ANN', 'is', 'based', 'on', 'a', 'collection', 'of', 'connected', 'units', 'or', 'nodes', 'called', 'artificial', 'neurons', 'which', 'loosely', 'model', 'the', 'neurons', 'in', 'a', 'biological', 'brain.In', 'common', 'ANN', 'implementations', ',', 'the', 'signal', 'at', 'a', 'connection', 'between', 'artificial', 'neurons', 'is', 'a', 'real', 'number', ',', 'and', 'the', 'output', 'of', 'each', 'artificial', 'neuron', 'is', 'computed', 'by', 'some', 'non-linear', 'function', 'of', 'the', 'sum', 'of', 'its', 'inputs', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLRx-QGDDFjA",
        "colab_type": "code",
        "outputId": "53ca14cd-10e1-42f1-ab93-2da229cd53dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data=pd.read_csv('emails.csv')\n",
        "data=data.values\n",
        "tfidfMat= TfidfVectorizer().fit_transform(data[:,0])\n",
        "tfidfMat.toarray()\n",
        "\n",
        "\n",
        "labels=data[:,1].astype('int')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidfMat, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "clf=GaussianNB()\n",
        "clf.fit(X_train.toarray(),y_train)\n",
        "\n",
        "print(classification_report(y_test, clf.predict(X_test.toarray())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0f259c5310d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emails.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtfidfMat\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'emails.csv' does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c659584f-4e3b-446d-8a3b-4e8e81a94458",
        "id": "GTP47iYR-XR0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#create dictionary\n",
        "doc1 = \"Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains\"\n",
        "doc2 = \"An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain.\"\n",
        "doc3 = \"In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.\"\n",
        "query = \"neuron in brain\"\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "import math \n",
        "\n",
        "def createdic(*docv):\n",
        "\n",
        "    temp = []\n",
        "    for doc in docv:\n",
        "          for word in doc.split(\" \"):\n",
        "              if word.lower() not in stop:\n",
        "                    temp.append(word.lower())\n",
        "    return temp\n",
        "\n",
        "\n",
        "vocab = createdic(doc1,doc2,doc3)\n",
        "print(vocab)\n",
        "\n",
        "#removing repeated terms \n",
        "\n",
        "def uniquelist(voc):\n",
        "  temp = []\n",
        "  for word in voc:\n",
        "    if word not in temp:\n",
        "      temp.append(word)\n",
        "  return list(set(temp))\n",
        "\n",
        "\n",
        "#dictionary = uniquelist(vocab)\n",
        "#print(dictionary)\n",
        "\n",
        "\n",
        "#now tf normalisation that is making the matrix \n",
        "from collections import Counter\n",
        "  \n",
        "def Frequency(wordlist):\n",
        "    from collections import Counter\n",
        "    dic = Counter(wordlist)\n",
        "    return dic\n",
        "\n",
        "def TermFrequency(vocab,*docv):\n",
        "    tf = []\n",
        "    for doc in docv:\n",
        "        temp = [] * len(vocab)\n",
        "        freq = Frequency(doc.lower().split(' '))\n",
        "        for word in vocab:\n",
        "          temp.append(freq[word])    \n",
        "        tf.append(temp)\n",
        "    return tf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "matrix = tfcounter(dictionary,doc1,doc2,doc3)\n",
        "print(matrix)\n",
        "N = 3\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "{'is', 'having', 'he', 'couldn', 'to', \"won't\", 'so', 'these', 'no', \"hadn't\", 'me', 'yourself', 'has', 'on', \"don't\", 'now', 'further', \"wasn't\", 'needn', 'other', 'some', \"shan't\", \"wouldn't\", 'most', 'their', 'been', \"couldn't\", 'weren', 'when', 'hers', 'didn', \"you've\", 'his', 'ain', 'won', 'here', 'them', 'in', 'it', \"should've\", 'how', 'its', 'of', 'which', 'don', 'herself', 'she', \"needn't\", 'between', 's', 'just', 'by', 'out', 'very', 'as', 'ma', 'before', 'y', 'hasn', 'yours', 'after', 'an', 'can', 'him', 'while', 'haven', 'too', 'why', \"you're\", 'few', 'own', 'more', 'not', 'they', 'for', \"haven't\", 'each', 'aren', 'but', \"mustn't\", \"aren't\", 'are', 'about', 'theirs', 'itself', 're', 'this', 'what', \"that'll\", 'into', 'down', 'hadn', 'off', 'if', 'myself', 'or', 'isn', 'did', 'himself', 'who', 'at', 'both', 'such', \"isn't\", 'wasn', \"you'd\", 'ours', \"she's\", 'doesn', 'mustn', 'be', 'because', 'until', \"shouldn't\", 'the', 'themselves', 'was', 'then', 'through', 'during', 'and', 'had', 'from', 'whom', 'll', 'were', 'i', 'below', 'do', 'all', 'any', 've', \"mightn't\", 'over', 'shan', \"didn't\", 'will', \"doesn't\", 'those', 'only', 'nor', 'a', \"hasn't\", 'yourselves', 'that', 'being', 'her', 'does', 't', 'than', 'your', 'there', 'have', 'with', 'doing', 'you', 'ourselves', 'up', 'under', 'once', 'm', \"it's\", 'against', 'd', 'o', 'shouldn', 'my', \"you'll\", 'where', \"weren't\", 'should', 'again', 'mightn', 'same', 'our', 'we', 'wouldn', 'am', 'above'}\n",
            "['artificial', 'neural', 'networks', '(ann)', 'connectionist', 'systems', 'computing', 'systems', 'vaguely', 'inspired', 'biological', 'neural', 'networks', 'constitute', 'animal', 'brains', 'ann', 'based', 'collection', 'connected', 'units', 'nodes', 'called', 'artificial', 'neurons', 'loosely', 'model', 'neurons', 'biological', 'brain.', 'common', 'ann', 'implementations,', 'signal', 'connection', 'artificial', 'neurons', 'real', 'number,', 'output', 'artificial', 'neuron', 'computed', 'non-linear', 'function', 'sum', 'inputs.']\n",
            "['called', 'systems', 'constitute', 'inspired', 'units', 'inputs.', 'artificial', 'connected', 'connectionist', 'implementations,', 'output', 'computed', 'connection', 'non-linear', 'neuron', 'function', 'collection', 'biological', 'common', 'nodes', 'real', 'neurons', 'networks', '(ann)', 'model', 'neural', 'sum', 'number,', 'brains', 'animal', 'ann', 'computing', 'vaguely', 'brain.', 'signal', 'loosely', 'based']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7a5d00012982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-86e9c554c51c>\u001b[0m in \u001b[0;36mtfcounter\u001b[0;34m(dic, *docv)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtemp1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WieVW61nHjWv",
        "colab_type": "code",
        "outputId": "df84197a-b75d-4ae7-f424-59ad5888f178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "#create dictionary\n",
        "doc1 = \"Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains\"\n",
        "doc2 = \"An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain.\"\n",
        "doc3 = \"In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.\"\n",
        "query = \"neuron in brain\"\n",
        "\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "import math \n",
        "\n",
        "def createdic(*docv):\n",
        "\n",
        "    temp = []\n",
        "    for doc in docv:\n",
        "          for word in doc.split(\" \"):\n",
        "              if word.lower() not in stop:\n",
        "                    temp.append(word.lower())\n",
        "    return temp\n",
        "\n",
        "\n",
        "vocab = createdic(doc1,doc2,doc3)\n",
        "print(vocab)\n",
        "\n",
        "#removing repeated terms \n",
        "\n",
        "def uniquelist(voc):\n",
        "  temp = []\n",
        "  for word in voc:\n",
        "    if word not in temp:\n",
        "      temp.append(word)\n",
        "  return list(set(temp))\n",
        "\n",
        "\n",
        "#dictionary = uniquelist(vocab)\n",
        "#print(dictionary)\n",
        "\n",
        "\n",
        "#now tf normalisation that is making the matrix \n",
        "from collections import Counter\n",
        "  \n",
        "def Frequency(wordlist):\n",
        "    from collections import Counter\n",
        "    dic = Counter(wordlist)\n",
        "    return dic\n",
        "\n",
        "def TermFrequency(vocab,*docv):\n",
        "    tf = []\n",
        "    for doc in docv:\n",
        "        temp = [] * len(vocab)\n",
        "        freq = Frequency(doc.lower().split(' '))\n",
        "        for word in vocab:\n",
        "          temp.append(freq[word])    \n",
        "        tf.append(temp)\n",
        "    return tf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "matrix = TermFrequency(dictionary,doc1,doc2,doc3)\n",
        "print(matrix)\n",
        "N = 3\n",
        "\n",
        "def idf \n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "{'is', 'having', 'he', 'couldn', 'to', \"won't\", 'so', 'these', 'no', \"hadn't\", 'me', 'yourself', 'has', 'on', \"don't\", 'now', 'further', \"wasn't\", 'needn', 'other', 'some', \"shan't\", \"wouldn't\", 'most', 'their', 'been', \"couldn't\", 'weren', 'when', 'hers', 'didn', \"you've\", 'his', 'ain', 'won', 'here', 'them', 'in', 'it', \"should've\", 'how', 'its', 'of', 'which', 'don', 'herself', 'she', \"needn't\", 'between', 's', 'just', 'by', 'out', 'very', 'as', 'ma', 'before', 'y', 'hasn', 'yours', 'after', 'an', 'can', 'him', 'while', 'haven', 'too', 'why', \"you're\", 'few', 'own', 'more', 'not', 'they', 'for', \"haven't\", 'each', 'aren', 'but', \"mustn't\", \"aren't\", 'are', 'about', 'theirs', 'itself', 're', 'this', 'what', \"that'll\", 'into', 'down', 'hadn', 'off', 'if', 'myself', 'or', 'isn', 'did', 'himself', 'who', 'at', 'both', 'such', \"isn't\", 'wasn', \"you'd\", 'ours', \"she's\", 'doesn', 'mustn', 'be', 'because', 'until', \"shouldn't\", 'the', 'themselves', 'was', 'then', 'through', 'during', 'and', 'had', 'from', 'whom', 'll', 'were', 'i', 'below', 'do', 'all', 'any', 've', \"mightn't\", 'over', 'shan', \"didn't\", 'will', \"doesn't\", 'those', 'only', 'nor', 'a', \"hasn't\", 'yourselves', 'that', 'being', 'her', 'does', 't', 'than', 'your', 'there', 'have', 'with', 'doing', 'you', 'ourselves', 'up', 'under', 'once', 'm', \"it's\", 'against', 'd', 'o', 'shouldn', 'my', \"you'll\", 'where', \"weren't\", 'should', 'again', 'mightn', 'same', 'our', 'we', 'wouldn', 'am', 'above'}\n",
            "['artificial', 'neural', 'networks', '(ann)', 'connectionist', 'systems', 'computing', 'systems', 'vaguely', 'inspired', 'biological', 'neural', 'networks', 'constitute', 'animal', 'brains', 'ann', 'based', 'collection', 'connected', 'units', 'nodes', 'called', 'artificial', 'neurons', 'loosely', 'model', 'neurons', 'biological', 'brain.', 'common', 'ann', 'implementations,', 'signal', 'connection', 'artificial', 'neurons', 'real', 'number,', 'output', 'artificial', 'neuron', 'computed', 'non-linear', 'function', 'sum', 'inputs.']\n",
            "[[0, 2, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1], [0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B38wNwQuGwH-",
        "colab_type": "code",
        "outputId": "343648e9-81b9-4aa4-a6a2-80748b5cf67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "doc1 = \"Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains\"\n",
        "doc2 = \"An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain.\"\n",
        "doc3 = \"In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.\"\n",
        "query = \"neuron in brain\"\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "def CreateVocab(*docv):\n",
        "  temp = []\n",
        "  for doc in docv:\n",
        "    temp.extend([i for i in word_tokenize(doc.lower()) if i not in stop])\n",
        "  return list(set(temp))\n",
        "\n",
        "def Frequency(wordlist):\n",
        "    from collections import Counter\n",
        "    dic = Counter(wordlist)\n",
        "    return dic\n",
        "\n",
        "def TermFrequency(vocab,*docv):\n",
        "    tf = []\n",
        "    for doc in docv:\n",
        "        temp = [] * len(vocab)\n",
        "        freq = Frequency(doc.lower().split(' '))\n",
        "        for word in vocab:\n",
        "          temp.append(freq[word])    \n",
        "        tf.append(temp)\n",
        "    return tf\n",
        "\n",
        "def InvDocFreq(vocab,*docv):\n",
        "  import math\n",
        "  N = len(docv)\n",
        "  idf = []\n",
        "  for word in vocab:\n",
        "    temp = 0\n",
        "    for doc in docv:\n",
        "      if word in doc.lower():\n",
        "        temp+=1\n",
        "    idf.append(math.log10(N/temp))\n",
        "  return idf\n",
        "\n",
        "def Tf_Idf(tf,idf):\n",
        "  tf_idf = []\n",
        "  for i in range(len(tf)):\n",
        "    temp = []\n",
        "    for j in range(len(idf)):\n",
        "      temp.append(tf[i][j]*idf[j])\n",
        "    tf_idf.append(temp)\n",
        "  return tf_idf\n",
        "\n",
        "\n",
        "def Tf_Idf(tf,idf):\n",
        "  tf_idf = []\n",
        "  for i in range(len(tf)):\n",
        "    temp = []\n",
        "    for j in range(len(idf)):\n",
        "      temp.append(tf[i][j]*idf[j])\n",
        "    tf_idf.append(temp)\n",
        "  return tf_idf\n",
        "\n",
        "vocab = CreateVocab(doc1,doc2,doc3)\n",
        "tf = TermFrequency(vocab,doc1,doc2,doc3)\n",
        "idf = InvDocFreq(vocab,doc1,doc2,doc3)\n",
        "tf_idf = Tf_Idf(tf,idf)\n",
        "print(vocab)\n",
        "print(tf)\n",
        "print(idf)\n",
        "print(tf_idf)\n",
        "\n",
        "tfq = TermFrequency(vocab,query)\n",
        "tf_idfq = Tf_Idf(tfq,idf)\n",
        "import numpy as np\n",
        "\n",
        "tf_idf = np.array(tf_idf)\n",
        "tf_idfq = np.array(tf_idfq)\n",
        "\n",
        "for i in range(3):\n",
        "  cos = np.dot(tf_idfq,tf_idf[i])/np.linalg.norm(tf_idf[i])*np.linalg.norm(tf_idfq)\n",
        "  print(\"Tf_Idf Doc {0} Cosine: {1}\".format(i+1,cos))\n",
        "\n",
        "for i in range(3):\n",
        "  cos = np.dot(tfq,tf[i])/np.linalg.norm(tf[i])*np.linalg.norm(tfq)\n",
        "  print(\"Tf Doc {0} Cosine: {1}\".format(i+1,cos))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['called', 'systems', '(', 'inputs', 'constitute', 'brain', 'inspired', 'units', 'artificial', 'connected', 'connectionist', 'output', 'implementations', 'connection', 'computed', 'non-linear', 'neuron', 'function', 'collection', 'biological', 'common', 'nodes', 'real', 'neurons', ')', 'networks', 'model', 'neural', 'sum', '.', 'brains', 'animal', 'ann', 'computing', 'vaguely', 'number', 'signal', 'loosely', ',', 'based']\n",
            "[[0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]\n",
            "[0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.0, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.0, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244, 0.47712125471966244]\n",
            "[[0.0, 0.9542425094393249, 0.0, 0.0, 0.47712125471966244, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17609125905568124, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9542425094393249, 0.0, 0.9542425094393249, 0.0, 0.0, 0.47712125471966244, 0.47712125471966244, 0.0, 0.47712125471966244, 0.47712125471966244, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47712125471966244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47712125471966244, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47712125471966244, 0.17609125905568124, 0.0, 0.47712125471966244, 0.0, 0.3521825181113625, 0.0, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47712125471966244, 0.0, 0.47712125471966244], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47712125471966244, 0.0, 0.17609125905568124, 0.47712125471966244, 0.47712125471966244, 0.17609125905568124, 0.47712125471966244, 0.0, 0.0, 0.47712125471966244, 0.0, 0.47712125471966244, 0.17609125905568124, 0.0, 0.0, 0.0, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47712125471966244, 0.0, 0.0, 0.0]]\n",
            "Tf_Idf Doc 1 Cosine: [0.]\n",
            "Tf_Idf Doc 2 Cosine: [0.]\n",
            "Tf_Idf Doc 3 Cosine: [0.00558132]\n",
            "Tf Doc 1 Cosine: [0.]\n",
            "Tf Doc 2 Cosine: [0.]\n",
            "Tf Doc 3 Cosine: [0.35355339]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}