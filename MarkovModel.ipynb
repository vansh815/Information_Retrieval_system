{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MarkovModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlbn1QLDvW7y",
        "colab_type": "text"
      },
      "source": [
        "# Word Prediction using Markov Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Ro2NLlvW77",
        "colab_type": "text"
      },
      "source": [
        "This notebook makes use of Markov model for word prediction. Specifically 2nd order Markov model is deployed here for next word prediction. As an example of the Markov chain, an attempt is made to generate a new song lyrics from a bunch of Eminem song lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsW-SqRcvW8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preamble\n",
        "import string\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxz1wSl1vW8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path of the text file containing the training data\n",
        "training_data_file = 'eminem_songs_lyrics.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4wS6hbQvW84",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9MX-Rm7vW87",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yTcoe-OvW8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(sentence):\n",
        "    return sentence.translate(str.maketrans('','', string.punctuation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdazWUtfvW9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add2dict(dictionary, key, value):\n",
        "    if key not in dictionary:\n",
        "        dictionary[key] = []\n",
        "    dictionary[key].append(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FRiQdy0vW9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list2probabilitydict(given_list):\n",
        "    probability_dict = {}\n",
        "    given_list_length = len(given_list)\n",
        "    for item in given_list:\n",
        "        probability_dict[item] = probability_dict.get(item, 0) + 1\n",
        "    for key, value in probability_dict.items():\n",
        "        probability_dict[key] = value / given_list_length\n",
        "    return probability_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIY4sHz0vW90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_word = {}\n",
        "second_word = {}\n",
        "transitions = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kAJuwZ3vW96",
        "colab_type": "text"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce1Lgfr0vW98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trains a Markov model based on the data in training_data_file\n",
        "def train_markov_model():\n",
        "    for line in open(training_data_file, encoding=\"utf8\"):\n",
        "        tokens = remove_punctuation(line.rstrip().lower()).split()\n",
        "        tokens_length = len(tokens)\n",
        "        for i in range(tokens_length):\n",
        "            token = tokens[i]\n",
        "            if i == 0:\n",
        "                initial_word[token] = initial_word.get(token, 0) + 1\n",
        "            else:\n",
        "                prev_token = tokens[i - 1]\n",
        "                if i == tokens_length - 1:\n",
        "                    add2dict(transitions, (prev_token, token), 'END')\n",
        "                if i == 1:\n",
        "                    add2dict(second_word, prev_token, token)\n",
        "                else:\n",
        "                    prev_prev_token = tokens[i - 2]\n",
        "                    add2dict(transitions, (prev_prev_token, prev_token), token)\n",
        "    \n",
        "    # Normalize the distributions\n",
        "    initial_word_total = sum(initial_word.values())\n",
        "    for key, value in initial_word.items():\n",
        "        initial_word[key] = value / initial_word_total\n",
        "        \n",
        "    for prev_word, next_word_list in second_word.items():\n",
        "        second_word[prev_word] = list2probabilitydict(next_word_list)\n",
        "        \n",
        "    for word_pair, next_word_list in transitions.items():\n",
        "        transitions[word_pair] = list2probabilitydict(next_word_list)\n",
        "    \n",
        "    print('Training successful.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6t0-EVzvW-W",
        "colab_type": "code",
        "outputId": "aa23ea21-089f-4158-e5a0-aa1a94773c8a",
        "colab": {}
      },
      "source": [
        "train_markov_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAZF4ZMYvW_D",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZdqs9nmvW_F",
        "colab_type": "text"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZGJczQvW_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_word(dictionary):\n",
        "    p0 = np.random.random()\n",
        "    cumulative = 0\n",
        "    for key, value in dictionary.items():\n",
        "        cumulative += value\n",
        "        if p0 < cumulative:\n",
        "            return key\n",
        "    assert(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoYgTXSdvW_P",
        "colab_type": "text"
      },
      "source": [
        "### Test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_cyovLxvW_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_sentences = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG8fCedMvW_Y",
        "colab_type": "code",
        "outputId": "38c141a2-26f0-4511-e2b4-67d7ea47837e",
        "colab": {}
      },
      "source": [
        "# Function to generate sample text\n",
        "print(initial_word)\n",
        "def generate():\n",
        "    for i in range(number_of_sentences):\n",
        "        sentence = []\n",
        "        # Initial word\n",
        "        word0 = sample_word(initial_word)\n",
        "        sentence.append(word0)\n",
        "        # Second word\n",
        "        word1 = sample_word(second_word[word0])\n",
        "        sentence.append(word1)\n",
        "        # Subsequent words untill END\n",
        "        while True:\n",
        "            word2 = sample_word(transitions[(word0, word1)])\n",
        "            if word2 == 'END':\n",
        "                break\n",
        "            sentence.append(word2)\n",
        "            word0 = word1\n",
        "            word1 = word2\n",
        "        print(' '.join(sentence))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'im': 0.023529411764705882, 'lifes': 0.002352941176470588, 'best': 0.002352941176470588, 'big': 0.002352941176470588, 'this': 0.018823529411764704, 'yella': 0.002352941176470588, 'immortality': 0.002352941176470588, 'no': 0.002352941176470588, 'too': 0.002352941176470588, 'basically': 0.002352941176470588, 'soon': 0.002352941176470588, 'make': 0.004705882352941176, 'bumping': 0.002352941176470588, 'uh': 0.002352941176470588, 'detergent': 0.002352941176470588, 'six': 0.004705882352941176, 'feet': 0.002352941176470588, 'why': 0.004705882352941176, 'after': 0.002352941176470588, 'full': 0.002352941176470588, 'lonely': 0.002352941176470588, 'easy': 0.002352941176470588, 'river': 0.004705882352941176, 'like': 0.004705882352941176, 'think': 0.002352941176470588, 'maybe': 0.002352941176470588, 'and': 0.05647058823529412, 'when': 0.007058823529411765, 'stay': 0.002352941176470588, 'til': 0.002352941176470588, 'in': 0.002352941176470588, 'everybody': 0.004705882352941176, 'feel': 0.004705882352941176, 'adrenaline': 0.002352941176470588, 'its': 0.01647058823529412, 'to': 0.009411764705882352, 'hi': 0.002352941176470588, 'angels': 0.002352941176470588, 'ever': 0.002352941176470588, 'thats': 0.007058823529411765, 'one': 0.002352941176470588, 'you': 0.04, 'always': 0.002352941176470588, 'simply': 0.002352941176470588, 'i': 0.047058823529411764, 'put': 0.002352941176470588, 'irreversible': 0.002352941176470588, 'backpack': 0.002352941176470588, 'with': 0.009411764705882352, 'on': 0.009411764705882352, 'look': 0.004705882352941176, 'let': 0.007058823529411765, 'of': 0.002352941176470588, 'add': 0.002352941176470588, 'i’ll': 0.002352941176470588, 'back': 0.002352941176470588, 'then': 0.002352941176470588, 'cause': 0.03529411764705882, 'somewhere': 0.002352941176470588, 'onenight': 0.002352941176470588, 'how': 0.009411764705882352, 'censor': 0.002352941176470588, 'mcs': 0.002352941176470588, 'his': 0.004705882352941176, 'just': 0.007058823529411765, 'unghh': 0.002352941176470588, 'so': 0.03294117647058824, 'lakim': 0.002352941176470588, 'dont': 0.011764705882352941, 'trying': 0.002352941176470588, 'blow': 0.002352941176470588, 'while': 0.004705882352941176, 'what': 0.009411764705882352, 'didnt': 0.002352941176470588, 'shes': 0.002352941176470588, 'lyrics': 0.002352941176470588, '\\ufeffive': 0.002352941176470588, 'theyre': 0.002352941176470588, 'roll': 0.002352941176470588, 'ill': 0.004705882352941176, 'somethings': 0.002352941176470588, 'sweat': 0.002352941176470588, 'syllables': 0.002352941176470588, 'they': 0.004705882352941176, 'theres': 0.002352941176470588, 'give': 0.002352941176470588, 'he': 0.018823529411764704, 'fact': 0.002352941176470588, 'mom': 0.002352941176470588, 'oh': 0.004705882352941176, 'that': 0.002352941176470588, 'well': 0.01411764705882353, 'man': 0.002352941176470588, 'made': 0.002352941176470588, 'oy': 0.002352941176470588, 'rappers': 0.004705882352941176, 'hit': 0.002352941176470588, 'baby': 0.002352941176470588, 'whats': 0.002352941176470588, 'never': 0.004705882352941176, 'ricocheting': 0.002352941176470588, 'kneel': 0.002352941176470588, 'throw': 0.002352941176470588, 'now': 0.009411764705882352, 'speeds': 0.002352941176470588, 'appeal': 0.002352941176470588, 'innovative': 0.002352941176470588, 'a': 0.004705882352941176, 'actually': 0.002352941176470588, 'even': 0.002352941176470588, 'into': 0.002352941176470588, 'coast': 0.002352941176470588, 'inspired': 0.002352941176470588, 'subliminal': 0.002352941176470588, 'off': 0.002352941176470588, 'little': 0.002352941176470588, 'teeter': 0.002352941176470588, 'says': 0.002352941176470588, 'for': 0.007058823529411765, 'feels': 0.002352941176470588, 'been': 0.009411764705882352, 'imma': 0.002352941176470588, 'all': 0.018823529411764704, 'see': 0.002352941176470588, 'did': 0.002352941176470588, 'hey': 0.004705882352941176, 'if': 0.01411764705882353, 'yo': 0.011764705882352941, 'packing': 0.002352941176470588, 'amoxacillin': 0.002352941176470588, 'immediately': 0.002352941176470588, 'it': 0.009411764705882352, 'morphin': 0.002352941176470588, 'get': 0.004705882352941176, 'snap': 0.002352941176470588, 'was': 0.002352941176470588, 'got': 0.002352941176470588, 'dale': 0.002352941176470588, 'truth': 0.002352941176470588, 'youre': 0.009411764705882352, 'the': 0.02823529411764706, 'my': 0.009411764705882352, 'took': 0.002352941176470588, 'till': 0.018823529411764704, 'enough': 0.002352941176470588, 'fcuk': 0.004705882352941176, 'tear': 0.002352941176470588, 'knife': 0.002352941176470588, 'call': 0.002352941176470588, 'your': 0.002352941176470588, 'that’s': 0.002352941176470588, 'tell': 0.002352941176470588, 'but': 0.058823529411764705, 'ima': 0.004705882352941176, 'would': 0.002352941176470588, 'prove': 0.002352941176470588, 'bet': 0.002352941176470588, 'at': 0.004705882352941176, 'still': 0.002352941176470588, 'we': 0.004705882352941176, 'heres': 0.004705882352941176, 'until': 0.004705882352941176, 'da': 0.002352941176470588, 'over': 0.002352941176470588, 'music': 0.002352941176470588, 'hes': 0.01411764705882353, 'me': 0.002352941176470588, 'only': 0.004705882352941176, 'ive': 0.011764705882352941, 'success': 0.002352941176470588}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoCt60E2vW_j",
        "colab_type": "text"
      },
      "source": [
        "### Testing arena"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yV05xdvvW_k",
        "colab_type": "code",
        "outputId": "2d8c1a74-e9b8-464c-c99a-71d0192a67d0",
        "colab": {}
      },
      "source": [
        "generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "until the roof comes off till the lights go out\n",
            "while he play piano\n",
            "fcuk can i say it was ironic\n",
            "trying to hold onto it\n",
            "well little one i dont want to admit to something\n",
            "feel weak you feel em\n",
            "i dont want to admit to something\n",
            "back to his mobile home thats when its\n",
            "feel my wrath of attack\n",
            "its curtains im inadvertently hurtin you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcZ2-k8LvW_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}